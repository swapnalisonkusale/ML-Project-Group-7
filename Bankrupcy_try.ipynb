{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa5ecc3",
   "metadata": {},
   "source": [
    "### Import necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5fdcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data processing modual\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Visualisation modual\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0904d",
   "metadata": {},
   "source": [
    "### Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfa749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sets the maximum number of columns and rows displayed when a frame is pretty-printed\n",
    "pd.set_option(\"max_rows\", None)\n",
    "pd.set_option(\"max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb42ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv('bankruptcy.csv')\n",
    "#Replace the categorical column (class) values into binary\n",
    "d['class']=pd.Categorical(d['class'].replace({\"b'0'\" : 0, \"b'1'\":1}))\n",
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ab968",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change the class variable from categorical to integer\n",
    "d['class']= pd.Categorical(d['class']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd105bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_Null_plot(dataset):\n",
    "    dataset.isnull().any() #check whether it is NULL for each row\n",
    "    msno.matrix(dataset, labels = True, sparkline = False, figsize=(35,20), fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb05b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using opened-source to show the plot\n",
    "show_Null_plot(d)\n",
    "\n",
    "# Find top 20 columns with most null values\n",
    "print(d.isnull().sum().sort_values(ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37012f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#replace the null values with 'nan'\n",
    "d = d.replace('', np.nan)\n",
    "nan_columns = []\n",
    "nan_values = []\n",
    "#count the number of  nan values in Dataset and plot a bargraph\n",
    "for column in d.columns:\n",
    "    nan_columns.append(column)\n",
    "    nan_values.append(d[column].isnull().sum())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "plt.bar(nan_columns, nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da44cb",
   "metadata": {},
   "source": [
    "Column Attr37 contains the highest number of \"Nan\" values and data imputation on the column will nto be useful. Remove the column and plot the bar graph again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb7abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete the column with highest number of nan values ie Column Attr37\n",
    "d = d.drop(columns = ['Attr37'])\n",
    "# Find 20 columns with most nan values\n",
    "print(d.isnull().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "show_Null_plot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485bf4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the null values graph after removing the Attr37 column\n",
    "nan_columns = []\n",
    "nan_values  = []\n",
    "\n",
    "for column in d.columns:\n",
    "    nan_columns.append(column)\n",
    "    nan_values.append(d[column].isnull().sum())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "plt.bar(nan_columns, nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbd4a8",
   "metadata": {},
   "source": [
    "## Outlier detection & Treatment\n",
    "Outliers are observations that are far away from the other data points in a random sample of a population. We often want to make assumptions about a specific population. Extreme values can have a significant impact on conclusions drawn from data.\n",
    "\n",
    "<b>Commonly used methods for detection of outliers is:</b> \n",
    "1. Tukey’s box plot method-\n",
    "    Tukey distinguishes between possible and probable outliers. A possible outlier is located between the inner and the outer       fence, whereas a probable outlier is located outside the outer fence.\n",
    "2. z-score method-\n",
    "    For each observation (Xn), it is measured how many standard deviations the data point is away from its mean (X̄).\n",
    "3. Median Absolute Deviation method-\n",
    "    Replaces the mean and standard deviation with more robust statistics, like the median and median absolute deviation. The       median absolute deviation is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7374832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Method for outlier detection via Inter-quartile range method: \n",
    "def outlier_detection(x, scale):\n",
    "    Q1 = x.quantile(0.25)\n",
    "    Q3 = x.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - scale * IQR \n",
    "    upper = Q3 + scale * IQR  \n",
    "    outliers = ((x<lower)| (x>upper)).sum()\n",
    "    total_length = len(x.index)\n",
    "    percent_outlier = (outliers * 100 / total_length).round(3)\n",
    "    print(x.name, \"\\tTotal outliers is: \", outliers,\"\\t\", \"\\t Percentage Outliers: \", percent_outlier )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20a0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check number of Outliers in all columns\n",
    "# Scale is 1.5 (Popular)\n",
    "scale = 1.5\n",
    "for column in d:\n",
    "    outlier_detection(d[column], scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467214ff",
   "metadata": {},
   "source": [
    "<b>For the treatement of outliers there are several methods:</b>\n",
    "1. Quantile-based Flooring and Capping:\n",
    "    Here points below 10th percentile are replaced with 10th percentile values\n",
    "    and  points above 90th percentile are replaced with 90th percentile values.\n",
    "    \n",
    "2. Trimming:\n",
    "      We completely remove data points that are outliers. This changes the total number of data rows present largely for our         bankruptcy data\n",
    "    \n",
    "3. IQR Score:\n",
    "    The rule of thumb is that anything not in the range of (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) is an outlier, and can be removed\n",
    "    \n",
    "4. Log Transformation:\n",
    "    Each cell could be transformed to a new value by formulating logarithmic, square, square root etc. But since we have           negative values too we cannot use this method\n",
    "    \n",
    "5. Replacing Outliers with Median Values:\n",
    "    We calculate the median and aassign those values to the upper and lower datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20bddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outlier treatment (Replacing Outliers with Median Values)\n",
    "df_median_treated = d.copy()\n",
    "def outlier_treatment(attr,threshold):\n",
    "    Q1 = attr.quantile(0.25)\n",
    "    Q2 = attr.quantile(0.50)\n",
    "    Q3 = attr.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - threshold * IQR \n",
    "    upper = Q3 + threshold * IQR \n",
    "    if (attr.name != 'class'):\n",
    "        df_median_treated[attr.name] = np.where((df_median_treated[attr.name]>upper)|(df_median_treated[attr.name]<lower), Q2 , df_median_treated[attr.name]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7135c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df_median_treated:\n",
    "    outlier_treatment(df_median_treated[column], scale)\n",
    "    \n",
    "df_median_treated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5222adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_median_treated.skew(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85b888",
   "metadata": {},
   "source": [
    "## Data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644da81e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data imputation via mean on outlier treated data\n",
    "d_imp1 = df_median_treated.copy()\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'mean') \n",
    "df_mean_imputed = pd.DataFrame(imp.fit_transform(d_imp1))\n",
    "df_mean_imputed.rename(columns={i:d_imp1.columns[i] for i in range(d_imp1.columns.size)}, inplace = True)   \n",
    "df_mean_imputed.describe().round(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701f699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data imputation via median on outlier treated data\n",
    "d_imp2 = df_median_treated.copy()\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'median') \n",
    "df_median_imputed= pd.DataFrame(imp.fit_transform(d_imp2))  # indexer method to update the dataframe\n",
    "df_median_imputed.rename(columns={i:d_imp2.columns[i] for i in range(d_imp2.columns.size)}, inplace = True)\n",
    "df_median_imputed.describe().round(5)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec79024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data imputation via mode on outlier treated data\n",
    "d_imp3 = df_median_treated.copy()\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent') \n",
    "df_freq_imputed = pd.DataFrame(imp.fit_transform(d_imp3))  # indexer method to update the dataframe\n",
    "df_freq_imputed.rename(columns={i:d_imp3.columns[i] for i in range(d_imp3.columns.size)}, inplace = True)\n",
    "df_freq_imputed.describe().round(5)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61377272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check Correlation matrix of mean imputed data\n",
    "corr = df_mean_imputed.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(13, 13))\n",
    "sns.set_style(style=\"white\")\n",
    "cmap = sns.diverging_palette(10, 250, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap,           #corr > 0.8 | corr < -0.8\n",
    "            square=True, linewidths=.5, \n",
    "            cbar_kws={\"shrink\": .5}, \n",
    "            ax= ax , \n",
    "            xticklabels = df_mean_imputed.columns.tolist(),\n",
    "            yticklabels = df_mean_imputed.columns.tolist()) \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a732d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check Correlation matrix of median imputed data\n",
    "\n",
    "corr_2 = df_median_imputed.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.set_style(style=\"white\")\n",
    "cmap = sns.diverging_palette(10, 250, as_cmap=True)\n",
    "sns.heatmap(corr>0.85, mask=mask,\n",
    "            cmap=cmap,\n",
    "            square=True, \n",
    "            linewidths=.5, \n",
    "            cbar_kws={\"shrink\": .5}, \n",
    "            ax= ax,\n",
    "            xticklabels = df_median_imputed.columns.tolist(),\n",
    "            yticklabels = df_median_imputed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7cd2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check Correlation matrix of mode imputed data\n",
    "\n",
    "corr = df_freq_imputed.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(15, 15))\n",
    "sns.set_style(style=\"white\")\n",
    "cmap = sns.diverging_palette(10, 250, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, \n",
    "            cmap=cmap,\n",
    "            square=True, \n",
    "            linewidths=.5,\n",
    "            cbar_kws={\"shrink\": .5},\n",
    "            ax= ax,\n",
    "            xticklabels = df_freq_imputed.columns.tolist(),\n",
    "            yticklabels = df_freq_imputed.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c366f9",
   "metadata": {},
   "source": [
    "Create the correlation groups and finalize the variables according to the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_group(correlation, percentile):\n",
    "    correlation_pairs = correlation.unstack()\n",
    "    high_correlation = correlation_pairs[(correlation_pairs>=percentile)|(correlation_pairs<= -1*percentile)]\n",
    "    print(high_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c75481",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_group(corr_2 , 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d27dc4",
   "metadata": {},
   "source": [
    "Columns with attributes 1,7,11,14,22,35,48 show positive correlation(>0.8)\n",
    "Columns with attributes 3,6,10,25,38,51    show negative correlation(<-0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d976e8c",
   "metadata": {},
   "source": [
    "Attributes positive correlation: \n",
    "1. Attr1 -net profit / total assets\n",
    "2. Attr7 -EBIT(Earnings before interest and taxes) / total assets\n",
    "3. Attr11-(gross profit + extraordinary items + financial expenses) / total assets\n",
    "4. Attr14-(gross profit + interest) / total assets\n",
    "5. Attr22- profit on operating activities / total assets\n",
    "6. Attr35-profit on sales / total assets\n",
    "7. Attr48-EBITDA (profit on operating activities - depreciation) / total assets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402342b6",
   "metadata": {},
   "source": [
    "Attributes negative correlation:\n",
    "1. Attr3 -working capital / total assets\n",
    "2. Attr6 -retained earnings / total assets\n",
    "3. Attr10-equity / total assets\n",
    "4. Attr25-(equity - share capital) / total assets\n",
    "5. Attr38-constant capital / total assets\n",
    "6. Attr51-short-term liabilities / total assets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778566f6",
   "metadata": {},
   "source": [
    "We take the above attributes in a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64baa945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new data frame with variables having high positive correlation\n",
    "df_positive_relation = df_median_imputed[[1,7,11,14,22,35,38,48, 63]]\n",
    "df_positive_relation.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f26e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df_positive_relation, labels =df_positive_relation.columns )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b674e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_positive_relation, hue= 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a6d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new data frame with variables having high negative correlation\n",
    "df_negative_relation = df_median_imputed[['Attr3','Attr6','Attr10','Attr25','Attr38','Attr51']]\n",
    "df_negative_relation.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b5765",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.boxplot(df_negative_relation, labels = df_negative_relation.columns )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4ac4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf72379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56096d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d7af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939456e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4b5f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22357420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee28ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd3021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ab1f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08e809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72431d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529bb9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
